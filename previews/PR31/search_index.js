var documenterSearchIndex = {"docs":
[{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"EditURL = \"<unknown>/howto_pinning.jl\"","category":"page"},{"location":"howtos/howto_pinning/#How-to-pin-Julia-threads","page":"Pinning Julia Threads","title":"How to pin Julia threads","text":"","category":"section"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Below, we demonstrate how to use LIKWID.jl to pin Julia threads to specific cores. However, before we do that, let us note two things.","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"note: Note\nInstead of LIKWID's pinning features, we generally strongly recommend to use ThreadPinning.jl to pin Julia threads to cores, as it provides many more options and visualizations!","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"note: Note\nNote that Julia implements task-based multithreading where N tasks get mapped onto M OS threads (M:N hybrid threading). We will pin the Julia (p)threads and not the tasks. Depending on how the latter are started/configured, tasks may migrate between Julia threads!","category":"page"},{"location":"howtos/howto_pinning/#Dynamic-pinning","page":"Pinning Julia Threads","title":"Dynamic pinning","text":"","category":"section"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"First, make sure to start Julia in multithreaded mode, i.e. julia -t N where N is the desired number of Julia threads (below I'll use N=10).","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"N = Threads.nthreads()","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"10","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Let's find out on which cores the Julia threads are currently running (before we've pinned them). We will use LIKWID.get_processor_id in combination with Threads.@threads :static for, which guarantees that the tasks associated with different instances of the loop body get executed on different Julia threads (ThreadPinning.jl provides @tspawnat as a nice(r) alternative).","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"using LIKWID\ncoreids = zeros(Int, N)\nThreads.@threads :static for i in 1:N\n    coreids[i] = LIKWID.get_processor_id()\nend\nprintln(\"Cores: \", coreids)","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Cores: [99, 109, 100, 111, 101, 104, 110, 102, 96, 108]\n","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Since querying all core ids is a common operation, we provide LIKWID.get_processor_ids which returns all core ids right away.","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"println(\"Cores: \", LIKWID.get_processor_ids())","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Cores: [99, 109, 100, 111, 101, 104, 110, 102, 96, 108]\n","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"To pin a thread to a specific core, there is LIKWID.pinthread. Using Threads.@threads :static for like above, we can, for example, pin the N Julia threads to the first N cores.","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"cores_firstN = 0:N-1\nThreads.@threads :static for i in 1:N\n    LIKWID.pinthread(cores_firstN[i])\nend\nprintln(\"Cores: \", LIKWID.get_processor_ids())","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Cores: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"To avoid the explicit for-loop, we can directly use LIKWID.pinthreads to pin all Julia threads. Let's realize a less trivial shuffled mapping.","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"using Random\ncores_firstN_shuffeled = shuffle(cores_firstN)\nLIKWID.pinthreads(cores_firstN_shuffeled)\nprintln(\"Cores: \", LIKWID.get_processor_ids())\nLIKWID.get_processor_ids() == cores_firstN_shuffeled","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"true","category":"page"},{"location":"howtos/howto_pinning/#likwid-pin","page":"Pinning Julia Threads","title":"likwid-pin","text":"","category":"section"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Command-line interface tool for pinning (p)threads. For details, check out the official documentation.","category":"page"},{"location":"howtos/howto_pinning/#Important:-Mask","page":"Pinning Julia Threads","title":"Important: Mask","text":"","category":"section"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"(See this discussion on the Julia discourse.)","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"In general, likwid-pin pins all pthreads. However, julia involves more than the \"Julia user threads\" specified via the -t option. For example, it create an additional unix signal thread (in src/signals-unix.c) and - unless OPENBLAS_NUM_THREADS=1 - the OpenBLAS related threads (blas_thread_init () in [..]/lib/julia/libopenblas64_.so). Hence, when you run likwid-pin -c 0-3 julia -t 4 the four cores (0-3) are actually oversubscribed and multiple \"Julia user threads\" get pinned to the same core.","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"To work around this, we need to provide a mask to likwid-pin via the -s option. To compute an appropriate mask for N \"Julia user threads\" you may use the helper function LIKWID.pinmask(N):","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"LIKWID.pinmask(4)","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"\"0xffffffffffffffe1\"","category":"page"},{"location":"howtos/howto_pinning/#Example","page":"Pinning Julia Threads","title":"Example","text":"","category":"section"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"(See https://github.com/JuliaPerf/LIKWID.jl/tree/main/examples/cli/likwid-pin/.)","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"# pin.jl\nusing Base.Threads\n\nglibc_coreid() = @ccall sched_getcpu()::Cint\n\n@threads :static for i in 1:nthreads()\n    println(\"Thread: $(i), CPU: $(glibc_coreid())\")\nend","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"Running this file with e.g. likwid-pin -s 0xffffffffffffffe1 -c 1,3,5,7 julia -t 4 pin.jl one obtains","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"[pthread wrapper]\n[pthread wrapper] MAIN -> 1\n[pthread wrapper] PIN_MASK: 0->3  1->5  2->7\n[pthread wrapper] SKIP MASK: 0xFFFFFFFFFFFFFFE1\n\tthreadid 140576878921280 -> SKIP\n\tthreadid 140576612378176 -> hwthread 3 - OK\n\tthreadid 140576590759488 -> hwthread 5 - OK\n\tthreadid 140576494188096 -> hwthread 7 - OK\nThread: 1, CPU: 1\nThread: 2, CPU: 3\nThread: 3, CPU: 5\nThread: 4, CPU: 7","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"If you're wondering about the -s 0xffffffffffffffe1 option, see Mask above.","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"","category":"page"},{"location":"howtos/howto_pinning/","page":"Pinning Julia Threads","title":"Pinning Julia Threads","text":"This page was generated using Literate.jl.","category":"page"},{"location":"references/topo_gpu/#GPU-Topology","page":"GPU topology","title":"GPU Topology","text":"","category":"section"},{"location":"references/topo_gpu/#Index","page":"GPU topology","title":"Index","text":"","category":"section"},{"location":"references/topo_gpu/","page":"GPU topology","title":"GPU topology","text":"Pages   = [\"topo_gpu.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"references/topo_gpu/#Functions","page":"GPU topology","title":"Functions","text":"","category":"section"},{"location":"references/topo_gpu/","page":"GPU topology","title":"GPU topology","text":"LIKWID.init_topology_gpu()\nLIKWID.finalize_topology_gpu()\nLIKWID.get_gpu_topology()","category":"page"},{"location":"references/topo_gpu/#LIKWID.init_topology_gpu-Tuple{}","page":"GPU topology","title":"LIKWID.init_topology_gpu","text":"Initialize LIKWIDs GPU topology module.\n\n\n\n\n\n","category":"method"},{"location":"references/topo_gpu/#LIKWID.finalize_topology_gpu-Tuple{}","page":"GPU topology","title":"LIKWID.finalize_topology_gpu","text":"Finalize LIKWIDs GPU topology module.\n\n\n\n\n\n","category":"method"},{"location":"references/topo_gpu/#LIKWID.get_gpu_topology-Tuple{}","page":"GPU topology","title":"LIKWID.get_gpu_topology","text":"Get GPU topology\n\n\n\n\n\n","category":"method"},{"location":"references/topo_gpu/#Types","page":"GPU topology","title":"Types","text":"","category":"section"},{"location":"references/topo_gpu/","page":"GPU topology","title":"GPU topology","text":"LIKWID.GpuTopology\nLIKWID.GpuDevice","category":"page"},{"location":"references/topo_gpu/#LIKWID.GpuTopology","page":"GPU topology","title":"LIKWID.GpuTopology","text":"Topology information of GPUs\n\n\n\n\n\n","category":"type"},{"location":"references/topo_gpu/#LIKWID.GpuDevice","page":"GPU topology","title":"LIKWID.GpuDevice","text":"Detailed information about a GPU device\n\n\n\n\n\n","category":"type"},{"location":"references/power/","page":"Power / Energy","title":"Power / Energy","text":"using LIKWID","category":"page"},{"location":"references/power/#Power-/-Energy","page":"Power / Energy","title":"Power / Energy","text":"","category":"section"},{"location":"references/power/#Example","page":"Power / Energy","title":"Example","text":"","category":"section"},{"location":"references/power/","page":"Power / Energy","title":"Power / Energy","text":"General power information:","category":"page"},{"location":"references/power/","page":"Power / Energy","title":"Power / Energy","text":"power = LIKWID.Power.get_power_info()\npower.domains\nfirst(power.domains)","category":"page"},{"location":"references/power/","page":"Power / Energy","title":"Power / Energy","text":"Energy measurement:","category":"page"},{"location":"references/power/","page":"Power / Energy","title":"Power / Energy","text":"LIKWID.Power.measure(; cpuid=0, domainid=0) do\n sleep(1)\nend\nLIKWID.Power.measure(; cpuid=0, domainid=0) do\n sum(sin(rand()) for _ in 1:1_000_000)\nend","category":"page"},{"location":"references/power/","page":"Power / Energy","title":"Power / Energy","text":"(Note that the example requires that the first (perhaps only) Julia thread is pinned to the CPU thread with id 0.)","category":"page"},{"location":"references/power/#Index","page":"Power / Energy","title":"Index","text":"","category":"section"},{"location":"references/power/","page":"Power / Energy","title":"Power / Energy","text":"Pages   = [\"power.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"references/power/#Functions","page":"Power / Energy","title":"Functions","text":"","category":"section"},{"location":"references/power/","page":"Power / Energy","title":"Power / Energy","text":"LIKWID.Power.init\nLIKWID.Power.finalize\nLIKWID.Power.get_power_info\nLIKWID.Power.start_power\nLIKWID.Power.stop_power\nLIKWID.Power.get_power\nLIKWID.Power.measure","category":"page"},{"location":"references/power/#LIKWID.Power.init","page":"Power / Energy","title":"LIKWID.Power.init","text":"Initialize power measurements for the given CPU. Returns the RAPL status, i.e. false (no RAPL) or true (RAPL working).\n\n\n\n\n\n","category":"function"},{"location":"references/power/#LIKWID.Power.finalize","page":"Power / Energy","title":"LIKWID.Power.finalize","text":"Finalize power measurements.\n\n\n\n\n\n","category":"function"},{"location":"references/power/#LIKWID.Power.get_power_info","page":"Power / Energy","title":"LIKWID.Power.get_power_info","text":"get_power_info() -> LIKWID.PowerInfo\n\nGet power / energy information.\n\n\n\n\n\n","category":"function"},{"location":"references/power/#LIKWID.Power.start_power","page":"Power / Energy","title":"LIKWID.Power.start_power","text":"Return the start value for a cpu (cpuid) for the domain with domainid.\n\n\n\n\n\n","category":"function"},{"location":"references/power/#LIKWID.Power.stop_power","page":"Power / Energy","title":"LIKWID.Power.stop_power","text":"Return the stop value for a cpu (cpuid) for the domain with domainid.\n\n\n\n\n\n","category":"function"},{"location":"references/power/#LIKWID.Power.get_power","page":"Power / Energy","title":"LIKWID.Power.get_power","text":"get_power(p_start::Integer, p_stop::Integer, domainid::Integer)\n\nCalculate the μJ from the values retrieved by start_power() and stop_power().\n\n\n\n\n\n","category":"function"},{"location":"references/power/#LIKWID.Power.measure","page":"Power / Energy","title":"LIKWID.Power.measure","text":"measure(f; cpuid::Integer=0, domainid::Integer)\n\nMeasure / calculate the energy for the given cpuid and domainid over the execution of the function f using Power.start_power, Power.stop_power, etc. under the hood. Automatically initializes and finalizes the power module.\n\nExamples\n\njulia> LIKWID.Power.measure(; cpuid=0, domainid=0) do\n           sleep(1)\n       end\n15.13702392578125 μJ\n\n\n\n\n\n","category":"function"},{"location":"references/power/#Types","page":"Power / Energy","title":"Types","text":"","category":"section"},{"location":"references/power/","page":"Power / Energy","title":"Power / Energy","text":"LIKWID.PowerInfo\nLIKWID.PowerDomain\nLIKWID.LibLikwid.PowerType\nLIKWID.TurboBoost","category":"page"},{"location":"references/power/#LIKWID.PowerInfo","page":"Power / Energy","title":"LIKWID.PowerInfo","text":"Power information\n\n\n\n\n\n","category":"type"},{"location":"references/power/#LIKWID.PowerDomain","page":"Power / Energy","title":"LIKWID.PowerDomain","text":"Power domain information\n\n\n\n\n\n","category":"type"},{"location":"references/power/#LIKWID.LibLikwid.PowerType","page":"Power / Energy","title":"LIKWID.LibLikwid.PowerType","text":"Different types of power domains\n\n\n\n\n\n","category":"type"},{"location":"references/power/#LIKWID.TurboBoost","page":"Power / Energy","title":"LIKWID.TurboBoost","text":"Turbo boost information\n\n\n\n\n\n","category":"type"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"EditURL = \"<unknown>/counting_flops.jl\"","category":"page"},{"location":"tutorials/counting_flops/#Counting-FLOPs","page":"Counting FLOPs","title":"Counting FLOPs","text":"","category":"section"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"Have you ever wondered how many floating point operations (FLOPs) a certain block of code, e.g. a Julia function, has actually triggered in a CPU core? With LIKWID.jl you can readily answer this question!","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"Let's consider a simple example: SAXPY. The abbreviation SAXPY stands for single-precision (Float32) a times x plus y, i.e. the computation","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"z = a cdot x + y","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"Of course, we can readily write this as a Julia function.","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"saxpy!(z, a, x, y) = z .= a .* x .+ y","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"saxpy! (generic function with 1 method)","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"Preparing some random input we can perform the saxpy! operation as per usual (we're suppressing the unimportant output below).","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"const N = 10_000\nconst a = 3.141\nconst x = rand(Float32, N)\nconst y = rand(Float32, N)\nconst z = zeros(Float32, N)\n\nsaxpy!(z, a, x, y);","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"Let's now use LIKWID to count the actually performed FLOPs for this computation! Concretely, we measure the FLOPS_SP performance group, in which \"SP\" stands for \"single precision\".","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"using LIKWID\nmetrics, events = @perfmon \"FLOPS_SP\" saxpy!(z, a, x, y)","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"(OrderedCollections.OrderedDict(\"FLOPS_SP\" => [OrderedCollections.OrderedDict(\"Runtime (RDTSC) [s]\" => 8.20070105282882e-6, \"Runtime unhalted [s]\" => 0.00033920369136497975, \"Clock [MHz]\" => 3498.110757920952, \"CPI\" => 1.5827581478264845, \"SP [MFLOP/s]\" => 2438.8158855151814), OrderedCollections.OrderedDict(\"Runtime (RDTSC) [s]\" => 8.20070105282882e-6, \"Runtime unhalted [s]\" => 4.20893123724612e-5, \"Clock [MHz]\" => 1819.3191133130201, \"CPI\" => NaN, \"SP [MFLOP/s]\" => 0.0), OrderedCollections.OrderedDict(\"Runtime (RDTSC) [s]\" => 8.20070105282882e-6, \"Runtime unhalted [s]\" => 4.353596665476168e-5, \"Clock [MHz]\" => 2828.7767234835533, \"CPI\" => NaN, \"SP [MFLOP/s]\" => 0.0), OrderedCollections.OrderedDict(\"Runtime (RDTSC) [s]\" => 8.20070105282882e-6, \"Runtime unhalted [s]\" => 3.96278774621136e-5, \"Clock [MHz]\" => 1909.3568239052859, \"CPI\" => NaN, \"SP [MFLOP/s]\" => 0.0)]), OrderedCollections.OrderedDict(\"FLOPS_SP\" => [OrderedCollections.OrderedDict(\"ACTUAL_CPU_CLOCK\" => 830978.0, \"MAX_CPU_CLOCK\" => 581949.0, \"RETIRED_INSTRUCTIONS\" => 21877.0, \"CPU_CLOCKS_UNHALTED\" => 34626.0, \"RETIRED_SSE_AVX_FLOPS_ALL\" => 20000.0, \"MERGE\" => 0.0), OrderedCollections.OrderedDict(\"ACTUAL_CPU_CLOCK\" => 103110.0, \"MAX_CPU_CLOCK\" => 138842.0, \"RETIRED_INSTRUCTIONS\" => 0.0, \"CPU_CLOCKS_UNHALTED\" => 0.0, \"RETIRED_SSE_AVX_FLOPS_ALL\" => 0.0, \"MERGE\" => 0.0), OrderedCollections.OrderedDict(\"ACTUAL_CPU_CLOCK\" => 106654.0, \"MAX_CPU_CLOCK\" => 92365.0, \"RETIRED_INSTRUCTIONS\" => 0.0, \"CPU_CLOCKS_UNHALTED\" => 0.0, \"RETIRED_SSE_AVX_FLOPS_ALL\" => 0.0, \"MERGE\" => 0.0), OrderedCollections.OrderedDict(\"ACTUAL_CPU_CLOCK\" => 97080.0, \"MAX_CPU_CLOCK\" => 124558.0, \"RETIRED_INSTRUCTIONS\" => 0.0, \"CPU_CLOCKS_UNHALTED\" => 0.0, \"RETIRED_SSE_AVX_FLOPS_ALL\" => 0.0, \"MERGE\" => 0.0)]))","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"That was easy. Let's see what we got and take a look at all measured (derived) metrics and (raw) events","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"first(metrics[\"FLOPS_SP\"])","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"OrderedCollections.OrderedDict{String, Float64} with 5 entries:\n  \"Runtime (RDTSC) [s]\" => 8.2007e-6\n  \"Runtime unhalted [s]\" => 0.000339204\n  \"Clock [MHz]\" => 3498.11\n  \"CPI\" => 1.58276\n  \"SP [MFLOP/s]\" => 2438.82","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"first(events[\"FLOPS_SP\"])","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"OrderedCollections.OrderedDict{String, Float64} with 6 entries:\n  \"ACTUAL_CPU_CLOCK\" => 830978.0\n  \"MAX_CPU_CLOCK\" => 581949.0\n  \"RETIRED_INSTRUCTIONS\" => 21877.0\n  \"CPU_CLOCKS_UNHALTED\" => 34626.0\n  \"RETIRED_SSE_AVX_FLOPS_ALL\" => 20000.0\n  \"MERGE\" => 0.0","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"Among all those entries, the event \"RETIRED_SSE_AVX_FLOPS_ALL\" is the one that we care about since it indicates the number of performed FLOPs.","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"NFLOPs_actual = first(events[\"FLOPS_SP\"])[\"RETIRED_SSE_AVX_FLOPS_ALL\"]","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"20000.0","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"note: Note\nUnfortunately, as CPUs can be very different the relevant event might have a different name on your system. Look out for something with \"FLOPS\" in events.","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"Let's check whether this number makes sense. Our vectors are of length N and for each element we perform two FLOPs in the SAXPY operation: one multiplication and one addition. Hence, our expectation is","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"NFLOPs_expected(N) = 2 * N\nNFLOPs_expected(N)","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"20000","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"Note that this perfectly matches our measurement result above!","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"NFLOPs_actual == NFLOPs_expected(N)","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"true","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"To rule out that this is just a big coincidence, let's try to modify N and check again. For convenience, let's wrap the above procedure into a function.","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"function count_FLOPs(N)\n    a = 3.141\n    x = rand(Float32, N)\n    y = rand(Float32, N)\n    z = zeros(Float32, N)\n    _, events = @perfmon \"FLOPS_SP\" saxpy!(z, a, x, y)\n    return first(events[\"FLOPS_SP\"])[\"RETIRED_SSE_AVX_FLOPS_ALL\"]\nend","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"count_FLOPs (generic function with 1 method)","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"See how it still matches our expectation when varying the input!","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"count_FLOPs(2 * N) == NFLOPs_expected(2 * N)","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"true","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"Feel free to play around further and apply this knowledge to other operations! As an inspiration: How many FLOPs does an exp.(x) or sin.(x) trigger? Does the answer depend on the length of x?","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"","category":"page"},{"location":"tutorials/counting_flops/","page":"Counting FLOPs","title":"Counting FLOPs","text":"This page was generated using Literate.jl.","category":"page"},{"location":"references/affinity/","page":"Affinity","title":"Affinity","text":"using LIKWID","category":"page"},{"location":"references/affinity/#Affinity","page":"Affinity","title":"Affinity","text":"","category":"section"},{"location":"references/affinity/#Example","page":"Affinity","title":"Example","text":"","category":"section"},{"location":"references/affinity/","page":"Affinity","title":"Affinity","text":"Query affinity domain information:","category":"page"},{"location":"references/affinity/","page":"Affinity","title":"Affinity","text":"aff = LIKWID.get_affinity()\naff.domains","category":"page"},{"location":"references/affinity/#Index","page":"Affinity","title":"Index","text":"","category":"section"},{"location":"references/affinity/","page":"Affinity","title":"Affinity","text":"Pages   = [\"affinity.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"references/affinity/#Functions","page":"Affinity","title":"Functions","text":"","category":"section"},{"location":"references/affinity/","page":"Affinity","title":"Affinity","text":"LIKWID.init_affinity\nLIKWID.finalize_affinity\nLIKWID.get_affinity\nLIKWID.cpustr_to_cpulist\nLIKWID.get_processor_id\nLIKWID.get_processor_ids\nLIKWID.get_processor_id_glibc\nLIKWID.pinprocess\nLIKWID.pinthread\nLIKWID.pinthreads","category":"page"},{"location":"references/affinity/#LIKWID.init_affinity","page":"Affinity","title":"LIKWID.init_affinity","text":"Initialize LIKWIDs affinity domain module.\n\n\n\n\n\n","category":"function"},{"location":"references/affinity/#LIKWID.finalize_affinity","page":"Affinity","title":"LIKWID.finalize_affinity","text":"Close and finalize LIKWIDs affinity domain module.\n\n\n\n\n\n","category":"function"},{"location":"references/affinity/#LIKWID.get_affinity","page":"Affinity","title":"LIKWID.get_affinity","text":"Query affinity domain information\n\n\n\n\n\n","category":"function"},{"location":"references/affinity/#LIKWID.cpustr_to_cpulist","page":"Affinity","title":"LIKWID.cpustr_to_cpulist","text":"Transform a valid cpu string in LIKWID syntax into a list of CPU IDs\n\n\n\n\n\n","category":"function"},{"location":"references/affinity/#LIKWID.get_processor_id","page":"Affinity","title":"LIKWID.get_processor_id","text":"Returns the ID of the currently executing CPU.\n\n\n\n\n\n","category":"function"},{"location":"references/affinity/#LIKWID.get_processor_ids","page":"Affinity","title":"LIKWID.get_processor_ids","text":"Get the CPU core IDs of the Julia threads.\n\n\n\n\n\n","category":"function"},{"location":"references/affinity/#LIKWID.get_processor_id_glibc","page":"Affinity","title":"LIKWID.get_processor_id_glibc","text":"Returns the ID of the currently executing CPU via glibcs sched_getcpu function.\n\n\n\n\n\n","category":"function"},{"location":"references/affinity/#LIKWID.pinprocess","page":"Affinity","title":"LIKWID.pinprocess","text":"Pins the current process to the CPU given as cpuid.\n\n\n\n\n\n","category":"function"},{"location":"references/affinity/#LIKWID.pinthread","page":"Affinity","title":"LIKWID.pinthread","text":"Pins the current thread to the CPU given as cpuid.\n\n\n\n\n\n","category":"function"},{"location":"references/affinity/#LIKWID.pinthreads","page":"Affinity","title":"LIKWID.pinthreads","text":"Pin all Julia threads to the CPU cores coreids. Note that length(coreids) == Threads.nthreads() must hold!\n\n\n\n\n\n","category":"function"},{"location":"references/affinity/#Types","page":"Affinity","title":"Types","text":"","category":"section"},{"location":"references/affinity/","page":"Affinity","title":"Affinity","text":"LIKWID.AffinityDomains\nLIKWID.AffinityDomain","category":"page"},{"location":"references/affinity/#LIKWID.AffinityDomains","page":"Affinity","title":"LIKWID.AffinityDomains","text":"Information about the affinity domains\n\n\n\n\n\n","category":"type"},{"location":"references/affinity/#LIKWID.AffinityDomain","page":"Affinity","title":"LIKWID.AffinityDomain","text":"An affinity domain\n\n\n\n\n\n","category":"type"},{"location":"references/perfmon/#Performance-Monitoring-(PerfMon)","page":"Performance monitoring","title":"Performance Monitoring (PerfMon)","text":"","category":"section"},{"location":"references/perfmon/#API","page":"Performance monitoring","title":"API","text":"","category":"section"},{"location":"references/perfmon/","page":"Performance monitoring","title":"Performance monitoring","text":"Pages   = [\"perfmon.md\"]\nOrder   = [:function, :macro, :type]","category":"page"},{"location":"references/perfmon/#Functions","page":"Performance monitoring","title":"Functions","text":"","category":"section"},{"location":"references/perfmon/","page":"Performance monitoring","title":"Performance monitoring","text":"Modules = [LIKWID.PerfMon]","category":"page"},{"location":"references/perfmon/#LIKWID.PerfMon.add_event_set-Tuple{AbstractString}","page":"Performance monitoring","title":"LIKWID.PerfMon.add_event_set","text":"add_event_set(estr) -> groupid\n\nAdd a performance group or a custom event set to the perfmon module. Returns a groupid (starting at 1) which is required to later specify the event set.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_event_results-Tuple{Any, Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_event_results","text":"get_event_results([groupid_or_groupname, eventid_or_eventname, threadid::Integer])\n\nRetrieve the results of monitored events. Same as get_metric_results but for raw events.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_id_of_active_group-Tuple{}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_id_of_active_group","text":"Return the groupid of the currently activate group.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_id_of_event-Tuple{Any, AbstractString}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_id_of_event","text":"Get the id of the event with the given name.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_id_of_group-Tuple{AbstractString}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_id_of_group","text":"Get the id of the group with the given name.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_id_of_metric-Tuple{Any, AbstractString}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_id_of_metric","text":"Get the id of the metric with the given name.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_last_metric-Tuple{Integer, Integer, Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_last_metric","text":"Return the derived metric result of the last measurement cycle identified by group groupid and the indices for metric metricidx and thread threadidx (all starting at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_last_result-Tuple{Integer, Integer, Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_last_result","text":"Return the raw counter register result of the last measurement cycle identified by group groupid and the indices for event eventidx and thread threadidx (all starting at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_longinfo_of_group-Tuple{Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_longinfo_of_group","text":"Return the (long) description of a performance group with id groupid (starts at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_metric-Tuple{Integer, Integer, Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_metric","text":"Return the derived metric result of all measurements identified by group groupid and the indices for metric metricidx and thread threadidx (all starting at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_metric_results-Tuple{Any, Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_metric_results","text":"get_metric_results([groupid_or_groupname, metricid_or_metricname, threadid::Integer])\n\nRetrieve the results of monitored metrics.\n\nOptionally, a group, metric, and threadid can be provided to select a subset of metrics or a single metric. If given as integers, note that groupid, metricid, and threadid all start at 1 and the latter enumerates the monitored cpu threads.\n\nIf no arguments are provided, a nested data structure is returned in which different levels correspond to performance groups, cpu threads, and metrics (in this order).\n\nExamples\n\njulia> PerfMon.get_metric_results(\"FLOPS_DP\")\n4-element Vector{OrderedDict{String, Float64}}:\n OrderedDict(\"Runtime (RDTSC) [s]\" => 1.1381168037989857, \"Runtime unhalted [s]\" => 0.0016642799007831007, \"Clock [MHz]\" => 2911.9285695819794, \"CPI\" => NaN, \"DP [MFLOP/s]\" => 0.0)\n OrderedDict(\"Runtime (RDTSC) [s]\" => 1.1381168037989857, \"Runtime unhalted [s]\" => 1.4755564705029072, \"Clock [MHz]\" => 3523.1114993407705, \"CPI\" => 0.3950777002592585, \"DP [MFLOP/s]\" => 17608.069202657578)\n OrderedDict(\"Runtime (RDTSC) [s]\" => 1.1381168037989857, \"Runtime unhalted [s]\" => 7.80437228993214e-5, \"Clock [MHz]\" => 2638.6244625814124, \"CPI\" => NaN, \"DP [MFLOP/s]\" => 0.0)\n OrderedDict(\"Runtime (RDTSC) [s]\" => 1.1381168037989857, \"Runtime unhalted [s]\" => 7.050705084934875e-5, \"Clock [MHz]\" => 2807.7525945849698, \"CPI\" => NaN, \"DP [MFLOP/s]\" => 0.0)\n\njulia> PerfMon.get_metric_results(\"FLOPS_DP\", 2) # results of second monitored cpu thread\nOrderedDict{String, Float64} with 5 entries:\n  \"Runtime (RDTSC) [s]\"  => 1.13812\n  \"Runtime unhalted [s]\" => 1.47556\n  \"Clock [MHz]\"          => 3523.11\n  \"CPI\"                  => 0.395078\n  \"DP [MFLOP/s]\"         => 17608.1\n\njulia> PerfMon.get_metric_results(\"FLOPS_DP\", \"DP [MFLOP/s]\", 2)\n17608.069202657578\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_metric_results-Tuple{}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_metric_results","text":"get_metric_results()\n\nGet the metric results for all performance groups and all monitored (PerfMon.init) cpu threads.\n\nReturns a an OrderedDict whose keys correspond to the performance groups and the values hold the results for all monitored cpu threads.\n\nExamples\n\njulia> results = PerfMon.get_metric_results()\nOrderedDict{String, Vector{OrderedDict{String, Float64}}} with 1 entry:\n  \"FLOPS_DP\" => [OrderedDict(\"Runtime (RDTSC) [s]\"=>1.13812, \"Runtime unhalted [s]\"=>0.00166428, \"Clock [MHz]\"=>291…\n\njulia> PerfMon.get_metric_results()[\"FLOPS_DP\"][2][\"DP [MFLOP/s]\"]\n17608.069202657578\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_name_of_counter-Tuple{Integer, Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_name_of_counter","text":"Return the name of the counter register identified by groupid and eventidx (both starting at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_name_of_event-Tuple{Integer, Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_name_of_event","text":"Return the name of the event identified by groupid and eventidx (both starting at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_name_of_group-Tuple{Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_name_of_group","text":"Return the name of the group identified by groupid (starts at 1). If it is a custom event set, the name is set to Custom.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_name_of_metric-Tuple{Integer, Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_name_of_metric","text":"Return the name of a derived metric identified by groupid and metricidx (both starting at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_number_of_events-Tuple{Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_number_of_events","text":"Return the amount of events in the given group with id groupid (starts at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_number_of_groups-Tuple{}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_number_of_groups","text":"Return the number of groups currently registered in the perfmon module.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_number_of_metrics-Tuple{Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_number_of_metrics","text":"Return the amount of metrics in the given group with id groupid (starts at 1). Always zero for custom event sets.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_number_of_threads-Tuple{}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_number_of_threads","text":"Return the number of threads initialized in the perfmon module.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_result-Tuple{Integer, Integer, Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_result","text":"Return the raw counter register result of all measurements identified by group groupid and the indices for event eventidx and thread threadidx (all starting at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_shortinfo_of_group-Tuple{Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_shortinfo_of_group","text":"Return the short information about a performance group with id groupid (starts at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.get_time_of_group-Tuple{Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.get_time_of_group","text":"Return the measurement time for group identified by groupid (starts at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.isgroupsupported-Tuple{Any}","page":"Performance monitoring","title":"LIKWID.PerfMon.isgroupsupported","text":"Checks if the given performance group is available on the current system.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.list_events-Tuple{Any}","page":"Performance monitoring","title":"LIKWID.PerfMon.list_events","text":"List all the events of a given group (groupid starts at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.list_metrics-Tuple{Any}","page":"Performance monitoring","title":"LIKWID.PerfMon.list_metrics","text":"List all the metrics of a given group (groupid starts at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.perfmon-Tuple{Any, Any}","page":"Performance monitoring","title":"LIKWID.PerfMon.perfmon","text":"perfmon(f, group_or_groups[; cpuids, autopin=true]) -> metrics, events\n\nMonitor performance groups while executing the given function f on one or multiple Julia threads. Note that\n\nPerfMon.init and PerfMon.finalize are called automatically\nthe measurement of multiple performance groups is sequential and requires multiple executions of f!\n\nThe returned data structures metrics and events are nested and different levels correspond to performance groups, threads, and measured metrics (in this order).\n\nKeyword arguments:\n\ncpuids (default: currently used CPU threads): specify the CPU threads (~ cores) to be monitored\nautopin (default: true): automatically pin Julia threads to the CPU threads (~ cores) they are currently running on (to avoid migration and wrong results).\n\nExample\n\njulia> using LIKWID\n\njulia> x = rand(1000); y = rand(1000);\n\njulia> metrics, events = perfmon(\"FLOPS_DP\") do\n           x .+ y;\n       end;\n\njulia> first(metrics[\"FLOPS_DP\"]) # all metrics of the first Julia thread\nOrderedDict{String, Float64} with 5 entries:\n  \"Runtime (RDTSC) [s]\"  => 8.56091e-6\n  \"Runtime unhalted [s]\" => 3.22377e-5\n  \"Clock [MHz]\"          => 3506.47\n  \"CPI\"                  => 4.78484\n  \"DP [MFLOP/s]\"         => 116.81\n\njulia> first(events[\"FLOPS_DP\"]) # all raw events of the first Julia thread\nOrderedDict{String, Float64} with 6 entries:\n  \"ACTUAL_CPU_CLOCK\"          => 78974.0\n  \"MAX_CPU_CLOCK\"             => 55174.0\n  \"RETIRED_INSTRUCTIONS\"      => 5977.0\n  \"CPU_CLOCKS_UNHALTED\"       => 28599.0\n  \"RETIRED_SSE_AVX_FLOPS_ALL\" => 1000.0\n  \"MERGE\"                     => 0.0\n\njulia> metrics, events = perfmon((\"FLOPS_DP\", \"MEM1\")) do\n           x .+ y;\n       end;\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.read_counters-Tuple{}","page":"Performance monitoring","title":"LIKWID.PerfMon.read_counters","text":"Read the counter registers. To be executed after start_counters and before stop_counters. Returns true on success.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.setup_counters-Tuple{Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.setup_counters","text":"Program the counter registers to measure all events in group groupid (starts at 1). Returns true on success.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.start_counters-Tuple{}","page":"Performance monitoring","title":"LIKWID.PerfMon.start_counters","text":"Start the counter registers. Returns true on success.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.stop_counters-Tuple{}","page":"Performance monitoring","title":"LIKWID.PerfMon.stop_counters","text":"Stop the counter registers. Returns true on success.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.supported_groups-Tuple{}","page":"Performance monitoring","title":"LIKWID.PerfMon.supported_groups","text":"Return a dictionary of all available perfmon groups.\n\nExamples\n\njulia> PerfMon.supported_groups()\nDict{String, LIKWID.GroupInfoCompact} with 18 entries:\n  \"L2CACHE\"  => L2CACHE => L2 cache miss rate/ratio (experimental)\n  \"MEM2\"     => MEM2 => Main memory bandwidth in MBytes/s (channels 4-7)\n  \"NUMA\"     => NUMA => L2 cache bandwidth in MBytes/s (experimental)\n  \"BRANCH\"   => BRANCH => Branch prediction miss rate/ratio\n  \"FLOPS_SP\" => FLOPS_SP => Single Precision MFLOP/s\n  \"DIVIDE\"   => DIVIDE => Divide unit information\n  \"CPI\"      => CPI => Cycles per instruction\n  \"L2\"       => L2 => L2 cache bandwidth in MBytes/s (experimental)\n  \"L3\"       => L3 => L3 cache bandwidth in MBytes/s\n  \"L3CACHE\"  => L3CACHE => L3 cache miss rate/ratio (experimental)\n  \"CACHE\"    => CACHE => Data cache miss rate/ratio\n  \"ICACHE\"   => ICACHE => Instruction cache miss rate/ratio\n  \"TLB\"      => TLB => TLB miss rate/ratio\n  \"CLOCK\"    => CLOCK => Cycles per instruction\n  \"FLOPS_DP\" => FLOPS_DP => Double Precision MFLOP/s\n  \"ENERGY\"   => ENERGY => Power and Energy consumption\n  \"MEM1\"     => MEM1 => Main memory bandwidth in MBytes/s (channels 0-3)\n  \"DATA\"     => DATA => Load to store ratio\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.switch_group-Tuple{Integer}","page":"Performance monitoring","title":"LIKWID.PerfMon.switch_group","text":"Switch currently active group to groupid (starts with 1). Returns true on success.\n\n\n\n\n\n","category":"method"},{"location":"references/perfmon/#LIKWID.PerfMon.@perfmon-Tuple{Any, Any}","page":"Performance monitoring","title":"LIKWID.PerfMon.@perfmon","text":"@perfmon group_or_groups codeblock\n\nSee also: perfmon\n\nExample\n\njulia> using LIKWID\n\njulia> x = rand(1000); y = rand(1000);\n\njulia> metrics, events = @perfmon \"FLOPS_DP\" x .+ y;\n\njulia> first(metrics[\"FLOPS_DP\"]) # all metrics of the first Julia thread\nOrderedDict{String, Float64} with 5 entries:\n  \"Runtime (RDTSC) [s]\"  => 8.56091e-6\n  \"Runtime unhalted [s]\" => 3.22377e-5\n  \"Clock [MHz]\"          => 3506.47\n  \"CPI\"                  => 4.78484\n  \"DP [MFLOP/s]\"         => 116.81\n\njulia> first(events[\"FLOPS_DP\"]) # all events of the first Julia thread\nOrderedDict{String, Float64} with 6 entries:\n  \"ACTUAL_CPU_CLOCK\"          => 78974.0\n  \"MAX_CPU_CLOCK\"             => 55174.0\n  \"RETIRED_INSTRUCTIONS\"      => 5977.0\n  \"CPU_CLOCKS_UNHALTED\"       => 28599.0\n  \"RETIRED_SSE_AVX_FLOPS_ALL\" => 1000.0\n  \"MERGE\"                     => 0.0\n\n\n\n\n\n","category":"macro"},{"location":"references/perfmon/#Types","page":"Performance monitoring","title":"Types","text":"","category":"section"},{"location":"references/perfmon/","page":"Performance monitoring","title":"Performance monitoring","text":"LIKWID.GroupInfoCompact","category":"page"},{"location":"references/perfmon/#LIKWID.GroupInfoCompact","page":"Performance monitoring","title":"LIKWID.GroupInfoCompact","text":"Essential information about a performance group\n\n\n\n\n\n","category":"type"},{"location":"references/access/#HPM-/-Access","page":"HPM / Access","title":"HPM / Access","text":"","category":"section"},{"location":"references/access/#Index","page":"HPM / Access","title":"Index","text":"","category":"section"},{"location":"references/access/","page":"HPM / Access","title":"HPM / Access","text":"Pages   = [\"access.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"references/access/#Functions","page":"HPM / Access","title":"Functions","text":"","category":"section"},{"location":"references/access/","page":"HPM / Access","title":"HPM / Access","text":"Modules = [LIKWID.HPM]","category":"page"},{"location":"references/access/#LIKWID.HPM.add_thread-Tuple{Any}","page":"HPM / Access","title":"LIKWID.HPM.add_thread","text":"Add the given CPU to the access module. This opens the commnunication to either the MSR/PCI files or the access daemon.\n\n\n\n\n\n","category":"method"},{"location":"references/access/#LIKWID.HPM.finalize-Tuple{}","page":"HPM / Access","title":"LIKWID.HPM.finalize","text":"Close the connections to the MSR/PCI files or the access daemon.\n\n\n\n\n\n","category":"method"},{"location":"references/access/#LIKWID.HPM.init-Tuple{}","page":"HPM / Access","title":"LIKWID.HPM.init","text":"Initialize the access module internals to either the MSR/PCI files or the access daemon\n\n\n\n\n\n","category":"method"},{"location":"references/access/#LIKWID.HPM.mode-Tuple{Union{LIKWID.LibLikwid.AccessMode, Integer}}","page":"HPM / Access","title":"LIKWID.HPM.mode","text":"Sets the mode how the MSR and PCI registers should be accessed. Available options:\n\n0 or LibLikwid.ACCESSMODE_DIRECT: direct access (propably root priviledges required)\n1 or LibLikwid.ACCESSMODE_DAEMON: accesses through the access daemon\n\nMust be called before HPM.init.\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#Marker-API-(CPU)","page":"Marker API (CPU)","title":"Marker API (CPU)","text":"","category":"section"},{"location":"references/marker/#Index","page":"Marker API (CPU)","title":"Index","text":"","category":"section"},{"location":"references/marker/","page":"Marker API (CPU)","title":"Marker API (CPU)","text":"Pages   = [\"marker.md\"]\nOrder   = [:function, :macro, :type]","category":"page"},{"location":"references/marker/#API","page":"Marker API (CPU)","title":"API","text":"","category":"section"},{"location":"references/marker/","page":"Marker API (CPU)","title":"Marker API (CPU)","text":"Modules = [Marker]","category":"page"},{"location":"references/marker/#LIKWID.Marker.close-Tuple{}","page":"Marker API (CPU)","title":"LIKWID.Marker.close","text":"Close the connection to the LIKWID Marker API and write out measurement data to file. This file will be evaluated by likwid-perfctr.\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#LIKWID.Marker.getregion-Tuple{AbstractString}","page":"Marker API (CPU)","title":"LIKWID.Marker.getregion","text":"getregion(regiontag::AbstractString, [num_events]) -> nevents, events, time, count\n\nGet the intermediate results of the region identified by regiontag. On success, it returns     * nevents: the number of events in the current group,     * events: a list with all the aggregated event results,     * time: the measurement time for the region and     * count: the number of calls.\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#LIKWID.Marker.init-Tuple{}","page":"Marker API (CPU)","title":"LIKWID.Marker.init","text":"Initialize the Marker API. Must be called previous to all other functions.\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#LIKWID.Marker.init_nothreads-Tuple{}","page":"Marker API (CPU)","title":"LIKWID.Marker.init_nothreads","text":"Initialize the Marker API only on the main thread. LIKWID.Marker.threadinit() must be called manually.\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#LIKWID.Marker.isactive-Tuple{}","page":"Marker API (CPU)","title":"LIKWID.Marker.isactive","text":"Checks whether the Marker API is active, i.e. julia has been started under likwid-perfctr -C ... -g ... -m.\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#LIKWID.Marker.nextgroup-Tuple{}","page":"Marker API (CPU)","title":"LIKWID.Marker.nextgroup","text":"Switch to the next event set in a round-robin fashion. If you have set only one event set on the command line, this function performs no operation.\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#LIKWID.Marker.region-Tuple{Any, AbstractString}","page":"Marker API (CPU)","title":"LIKWID.Marker.region","text":"region(f, regiontag::AbstractString)\n\nAdds a LIKWID marker region around the execution of the given function f using Marker.startregion, Marker.stopregion under the hood. Note that LIKWID.Marker.init() and LIKWID.Marker.close() must be called before and after, respectively.\n\nExamples\n\njulia> using LIKWID\n\njulia> Marker.init()\n\njulia> region(\"sleeping...\") do\n           sleep(1)\n       end\ntrue\n\njulia> region(()->rand(100), \"create rand vec\")\ntrue\n\njulia> Marker.close()\n\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#LIKWID.Marker.registerregion-Tuple{AbstractString}","page":"Marker API (CPU)","title":"LIKWID.Marker.registerregion","text":"Register a region with name regiontag to the Marker API. On success, true is returned.\n\nThis is an optional function to reduce the overhead of region registration at Marker.startregion. If you don't call registerregion, the registration is done at startregion.\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#LIKWID.Marker.resetregion-Tuple{AbstractString}","page":"Marker API (CPU)","title":"LIKWID.Marker.resetregion","text":"Reset the values stored using the region name regiontag. On success, true is returned.\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#LIKWID.Marker.startregion-Tuple{AbstractString}","page":"Marker API (CPU)","title":"LIKWID.Marker.startregion","text":"Start measurements under the name regiontag. On success, true is returned.\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#LIKWID.Marker.stopregion-Tuple{AbstractString}","page":"Marker API (CPU)","title":"LIKWID.Marker.stopregion","text":"Stop measurements under the name regiontag. On success, true is returned.\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#LIKWID.Marker.threadinit-Tuple{}","page":"Marker API (CPU)","title":"LIKWID.Marker.threadinit","text":"Add the current thread to the Marker API.\n\n\n\n\n\n","category":"method"},{"location":"references/marker/#LIKWID.Marker.@parallelregion-Tuple{Any, Any}","page":"Marker API (CPU)","title":"LIKWID.Marker.@parallelregion","text":"Convenience macro for flanking code with Marker.startregion and Marker.stopregion on all threads separately.\n\nExamples\n\njulia> using LIKWID\n\njulia> Marker.init()\n\njulia> @parallelregion begin\n           Threads.@thread :static for i in 1:Threads.nthreads()\n               # thread-local computation\n           end\n       end\n\njulia> Marker.close()\n\n\n\n\n\n\n","category":"macro"},{"location":"references/marker/#LIKWID.Marker.@region-Tuple{Any, Any}","page":"Marker API (CPU)","title":"LIKWID.Marker.@region","text":"Convenience macro for flanking code with Marker.startregion and Marker.stopregion.\n\nExamples\n\njulia> using LIKWID\n\njulia> Marker.init()\n\njulia> @region \"sleeping...\" sleep(1)\ntrue\n\njulia> @region \"create rand vec\" rand(100)\ntrue\n\njulia> Marker.close()\n\n\n\n\n\n\n","category":"macro"},{"location":"howtos/howto_perfmon/","page":"Performance Monitoring","title":"Performance Monitoring","text":"EditURL = \"<unknown>/howto_perfmon.jl\"","category":"page"},{"location":"howtos/howto_perfmon/","page":"Performance Monitoring","title":"Performance Monitoring","text":"TODO","category":"page"},{"location":"howtos/howto_perfmon/","page":"Performance Monitoring","title":"Performance Monitoring","text":"","category":"page"},{"location":"howtos/howto_perfmon/","page":"Performance Monitoring","title":"Performance Monitoring","text":"This page was generated using Literate.jl.","category":"page"},{"location":"references/timer/","page":"CPU clock timer","title":"CPU clock timer","text":"using LIKWID","category":"page"},{"location":"references/timer/#CPU-Clock-Timer","page":"CPU clock timer","title":"CPU Clock Timer","text":"","category":"section"},{"location":"references/timer/#Example","page":"CPU clock timer","title":"Example","text":"","category":"section"},{"location":"references/timer/","page":"CPU clock timer","title":"CPU clock timer","text":"Timing is as simple as","category":"page"},{"location":"references/timer/","page":"CPU clock timer","title":"CPU clock timer","text":"LIKWID.Timer.@timeit sleep(1)","category":"page"},{"location":"references/timer/","page":"CPU clock timer","title":"CPU clock timer","text":"Apart from the time it took to execute sleep(1) (clock) one also gets the number of CPU clock cycles corresponding to the time interval (cycles).","category":"page"},{"location":"references/timer/","page":"CPU clock timer","title":"CPU clock timer","text":"Note that the macro usage above is essentially equivalent to the following manual sequence","category":"page"},{"location":"references/timer/","page":"CPU clock timer","title":"CPU clock timer","text":"LIKWID.Timer.init()\nstart = LIKWID.Timer.start_clock()\nsleep(1)\nt_stop = LIKWID.Timer.stop_clock(t_start)\nLIKWID.Timer.get_clock(t_stop)\nLIKWID.Timer.get_clock_cycles(t_stop)\nLIKWID.Timer.finalize()","category":"page"},{"location":"references/timer/#Index","page":"CPU clock timer","title":"Index","text":"","category":"section"},{"location":"references/timer/","page":"CPU clock timer","title":"CPU clock timer","text":"Pages   = [\"timer.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"references/timer/#Functions","page":"CPU clock timer","title":"Functions","text":"","category":"section"},{"location":"references/timer/","page":"CPU clock timer","title":"CPU clock timer","text":"LIKWID.Timer.init\nLIKWID.Timer.finalize\nLIKWID.Timer.get_cpu_clock\nLIKWID.Timer.get_cpu_clock_current\nLIKWID.Timer.start_clock\nLIKWID.Timer.stop_clock\nLIKWID.Timer.get_clock\nLIKWID.Timer.get_clock_cycles\nLIKWID.Timer.timeit\nLIKWID.Timer.@timeit","category":"page"},{"location":"references/timer/#LIKWID.Timer.init","page":"CPU clock timer","title":"LIKWID.Timer.init","text":"Initialize LIKWIDs timer module\n\n\n\n\n\n","category":"function"},{"location":"references/timer/#LIKWID.Timer.finalize","page":"CPU clock timer","title":"LIKWID.Timer.finalize","text":"Close and finalize LIKWIDs timer module\n\n\n\n\n\n","category":"function"},{"location":"references/timer/#LIKWID.Timer.get_cpu_clock","page":"CPU clock timer","title":"LIKWID.Timer.get_cpu_clock","text":"Return the CPU clock determined at Timer.init().\n\n\n\n\n\n","category":"function"},{"location":"references/timer/#LIKWID.Timer.get_cpu_clock_current","page":"CPU clock timer","title":"LIKWID.Timer.get_cpu_clock_current","text":"Return the current CPU clock read from sysfs\n\n\n\n\n\n","category":"function"},{"location":"references/timer/#LIKWID.Timer.start_clock","page":"CPU clock timer","title":"LIKWID.Timer.start_clock","text":"Start the clock and return a LibLikwid.TimerData object including the start timestamp.\n\n\n\n\n\n","category":"function"},{"location":"references/timer/#LIKWID.Timer.stop_clock","page":"CPU clock timer","title":"LIKWID.Timer.stop_clock","text":"stop_clock(timer::LibLikwid.TimerData) -> newtimer::LibLikwid.TimerData\n\nStop the clock and return a LibLikwid.TimerData object including the start and stop timestamps. The input timer should be the output of Timer.start_clock().\n\n\n\n\n\n","category":"function"},{"location":"references/timer/#LIKWID.Timer.get_clock","page":"CPU clock timer","title":"LIKWID.Timer.get_clock","text":"get_clock(timer::LibLikwid.TimerData)\n\nReturn the measured interval in seconds for the given timer. The input timer should be the output of Timer.stop_clock.\n\n\n\n\n\n","category":"function"},{"location":"references/timer/#LIKWID.Timer.get_clock_cycles","page":"CPU clock timer","title":"LIKWID.Timer.get_clock_cycles","text":"get_clock_cycles(timer::LibLikwid.TimerData)\n\nReturn the measured interval in cycles for the given timer. The input timer should be the output of Timer.stop_clock.\n\n\n\n\n\n","category":"function"},{"location":"references/timer/#LIKWID.Timer.timeit","page":"CPU clock timer","title":"LIKWID.Timer.timeit","text":"timeit(f)\n\nTime the given function f using Timer.start_clock, Timer.stop_clock, etc. under the hood. Automatically initializes and finalizes the timer module.\n\nExamples\n\njulia> LIKWID.Timer.timeit() do\n           sleep(1)\n       end\n(clock = 1.0008815780376372, cycles = 3603224844)\n\n\n\n\n\n","category":"function"},{"location":"references/timer/#LIKWID.Timer.@timeit","page":"CPU clock timer","title":"LIKWID.Timer.@timeit","text":"Convenience macro for Timer.timeit.\n\nExamples\n\njulia> LIKWID.Timer.@timeit sleep(1)\n(clock = 1.0008815780376372, cycles = 3603224844)\n\n\n\n\n\n","category":"macro"},{"location":"references/topo/#CPU-/-NUMA-Topology","page":"CPU topology","title":"CPU / NUMA Topology","text":"","category":"section"},{"location":"references/topo/#Index","page":"CPU topology","title":"Index","text":"","category":"section"},{"location":"references/topo/","page":"CPU topology","title":"CPU topology","text":"Pages   = [\"topo.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"references/topo/#Functions","page":"CPU topology","title":"Functions","text":"","category":"section"},{"location":"references/topo/","page":"CPU topology","title":"CPU topology","text":"LIKWID.init_topology\nLIKWID.finalize_topology\nLIKWID.get_cpu_topology\nLIKWID.get_cpu_info\nLIKWID.print_supported_cpus\nLIKWID.init_numa\nLIKWID.finalize_numa\nLIKWID.get_numa_topology","category":"page"},{"location":"references/topo/#LIKWID.init_topology","page":"CPU topology","title":"LIKWID.init_topology","text":"Initialize LIKWIDs topology module.\n\n\n\n\n\n","category":"function"},{"location":"references/topo/#LIKWID.finalize_topology","page":"CPU topology","title":"LIKWID.finalize_topology","text":"Close and finalize LIKWIDs topology module.\n\n\n\n\n\n","category":"function"},{"location":"references/topo/#LIKWID.get_cpu_topology","page":"CPU topology","title":"LIKWID.get_cpu_topology","text":"get_cpu_topology() -> CpuTopology\n\nGet the CPU topology of the machine.\n\nAutomatically initializes the topology and NUMA modules, i.e. calls LIKWID.init_topology and LIKWID.init_numa.\n\n\n\n\n\n","category":"function"},{"location":"references/topo/#LIKWID.get_cpu_info","page":"CPU topology","title":"LIKWID.get_cpu_info","text":"get_cpu_info() -> CpuInfo\n\nGet detailed information about the CPU.\n\nAutomatically initializes the topology and NUMA modules, i.e. calls LIKWID.init_topology and LIKWID.init_numa.\n\n\n\n\n\n","category":"function"},{"location":"references/topo/#LIKWID.print_supported_cpus","page":"CPU topology","title":"LIKWID.print_supported_cpus","text":"print_supported_cpus(; cprint=true)\n\nPrint a list of all supported CPUs.\n\nIf cprint=false, LIKWID.jl will first capture the stdout and then print the list.\n\n\n\n\n\n","category":"function"},{"location":"references/topo/#LIKWID.init_numa","page":"CPU topology","title":"LIKWID.init_numa","text":"Initialize LIKWIDs NUMA module.\n\n\n\n\n\n","category":"function"},{"location":"references/topo/#LIKWID.finalize_numa","page":"CPU topology","title":"LIKWID.finalize_numa","text":"Close and finalize LIKWIDs NUMA module.\n\n\n\n\n\n","category":"function"},{"location":"references/topo/#LIKWID.get_numa_topology","page":"CPU topology","title":"LIKWID.get_numa_topology","text":"get_numa_topology() -> NumaTopology\n\nGet the NUMA topology of the machine.\n\nAutomatically initializes the topology, NUMA, and affinity modules, i.e. calls LIKWID.init_topology, LIKWID.init_numa, and LIKWID.init_affinity.\n\n\n\n\n\n","category":"function"},{"location":"references/topo/#Types","page":"CPU topology","title":"Types","text":"","category":"section"},{"location":"references/topo/","page":"CPU topology","title":"CPU topology","text":"LIKWID.CpuTopology\nLIKWID.CpuInfo\nLIKWID.HWThread\nLIKWID.CacheLevel\nLIKWID.NumaTopology\nLIKWID.NumaNode","category":"page"},{"location":"references/topo/#LIKWID.CpuTopology","page":"CPU topology","title":"LIKWID.CpuTopology","text":"CPU topology information\n\n\n\n\n\n","category":"type"},{"location":"references/topo/#LIKWID.CpuInfo","page":"CPU topology","title":"LIKWID.CpuInfo","text":"CPU information\n\n\n\n\n\n","category":"type"},{"location":"references/topo/#LIKWID.HWThread","page":"CPU topology","title":"LIKWID.HWThread","text":"Information about a hardware thread\n\n\n\n\n\n","category":"type"},{"location":"references/topo/#LIKWID.CacheLevel","page":"CPU topology","title":"LIKWID.CacheLevel","text":"Information about a cache level\n\n\n\n\n\n","category":"type"},{"location":"references/topo/#LIKWID.NumaTopology","page":"CPU topology","title":"LIKWID.NumaTopology","text":"CPU topology information\n\n\n\n\n\n","category":"type"},{"location":"references/topo/#LIKWID.NumaNode","page":"CPU topology","title":"LIKWID.NumaNode","text":"Information about a NUMA node\n\n\n\n\n\n","category":"type"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"EditURL = \"<unknown>/howto_marker_cpu_dynamic.jl\"","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/#Marker-API-(CPU):-Dynamic-Usage","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"","category":"section"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"This is a demo of how to use the marker API to monitor the performance of a computation (do_flops below) running on multiple Julia threads using LIKWID from within Julia (i.e. without using likwid-perfctr ...). You can simply start Julia with julia -t N.","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/#Setting-up","page":"Marker API (CPU): Dynamic Usage","title":"Setting up","text":"","category":"section"},{"location":"howtos/howto_marker_cpu_dynamic/#Pinning-the-Julia-threads","page":"Marker API (CPU): Dynamic Usage","title":"Pinning the Julia threads","text":"","category":"section"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"It's absolutely necessary to pin the Julia threads to specific cores. Otherwise, the threads might migrate to different cores and our hardware performance counter measurements are meaningless.","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"Let's pin the Julia threads to the first nthreads() cores.","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"using LIKWID\nusing Base.Threads: @threads, nthreads\n@assert nthreads() > 1 # hide\nLIKWID.pinthreads(0:nthreads()-1)","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/#Environment-variables","page":"Marker API (CPU): Dynamic Usage","title":"Environment variables","text":"","category":"section"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"We use the LIKWID.LIKWID_* functions to set environment variables to configure LIKWID for our monitoring.","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"# use the following threads\ncpustr = join(0:nthreads()-1, \",\")\nLIKWID.LIKWID_THREADS(cpustr)\n# the location the marker file will be stored\nLIKWID.LIKWID_FILEPATH(joinpath(@__DIR__, \"likwid_marker.out\"))\n# Use the access daemon\nLIKWID.LIKWID_MODE(1)\n# Overwrite registers (if they are in use)\nLIKWID.LIKWID_FORCE(true)\n# Debug level\nLIKWID.LIKWID_DEBUG(0)\n# Events to measure\nLIKWID.LIKWID_EVENTS(\"FLOPS_DP|L2|INSTR_RETIRED_ANY:FIXC0\");","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/#Initialize-LIKWID-modules","page":"Marker API (CPU): Dynamic Usage","title":"Initialize LIKWID modules","text":"","category":"section"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"using LIKWID: PerfMon\nMarker.init_nothreads()\nPerfMon.init()","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"true","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/#Measurement","page":"Marker API (CPU): Dynamic Usage","title":"Measurement","text":"","category":"section"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"# simple function designed to do floating point computations\nfunction do_flops(a, b, c, num_flops)\n    for _ in 1:num_flops\n        c = a * b + c\n    end\n    return c\nend","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"do_flops (generic function with 1 method)","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"Let's run the computation and monitor the performance. For good measure, we put everything in a function.","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"function monitor_do_flops(NUM_FLOPS = 100_000_000)\n    a = 1.8\n    b = 3.2\n    c = 1.0\n    @threads :static for tid in 1:nthreads()\n        # Notice that only the first group specified, `FLOPS_DP`, will be measured.\n        # See further below for how to measure multiple groups.\n        ; Marker.startregion(\"calc_flops\")\n        @region \"calc_flops\" c = do_flops(c, a, b, NUM_FLOPS)\n        ; Marker.stopregion(\"calc_flops\")\n    end\n    return nothing\nend\nmonitor_do_flops()","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"WARN: Region calc_flops was already started\nWARN: Region calc_flops was already started\nWARN: Region calc_flops was already started\nWARN: Stopping an unknown/not-started region calc_flops\nWARN: Stopping an unknown/not-started region calc_flops\nWARN: Stopping an unknown/not-started region calc_flops\n","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/#Analysis","page":"Marker API (CPU): Dynamic Usage","title":"Analysis","text":"","category":"section"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"To query basic information about the region from all threads we use Marker.getregion.","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"@threads :static for threadid in 1:nthreads()\n    nevents, events, time, count = Marker.getregion(\"calc_flops\")\n    gid = PerfMon.get_id_of_active_group()\n    group_name = PerfMon.get_name_of_group(gid)\n    # print basic info\n    println(\"Thread $(threadid): group $(group_name), $(nevents) events, runtime $(time) s, and call count $(count)\")\nend;","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"Thread 1: group FLOPS_DP, 6 events, runtime 0.09083124763678302 s, and call count 1\nThread 2: group FLOPS_DP, 6 events, runtime 0.09081997779150272 s, and call count 1\nThread 3: group FLOPS_DP, 6 events, runtime 0.09083271784730416 s, and call count 1\n","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"The tools from the LIKWID.PerfMon module can be used to get more detailed information, such as the event and metric results.","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"using PrettyTables\nusing Printf\n_zeroifnothing(x::Nothing) = 0.0\n_zeroifnothing(x) = x\n\n# extract event and metric results\ngid = PerfMon.get_id_of_active_group()\nnevents = PerfMon.get_number_of_events(gid)\nnmetrics = PerfMon.get_number_of_metrics(gid)\nevents = Matrix(undef, nevents, nthreads() + 1)\nmetrics = Matrix(undef, nmetrics, nthreads() + 1)\n\nfor tid in 1:nthreads()\n    for eid in 1:nevents\n        events[eid, 1] = PerfMon.get_name_of_event(gid, eid)\n        events[eid, tid+1] = _zeroifnothing(PerfMon.get_result(gid, eid, tid))\n    end\n    for mid in 1:nmetrics\n        metrics[mid, 1] = PerfMon.get_name_of_metric(gid, mid)\n        metrics[mid, tid+1] = _zeroifnothing(PerfMon.get_metric(gid, mid, tid))\n    end\nend\n\n# printing\ntheader = [\"Thread $(i)\" for i in 1:nthreads()]\npretty_table(events; header = vcat([\"Event\"], theader))\npretty_table(metrics; header = vcat([\"Metric\"], theader))","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"┌───────────────────────────┬───────────┬───────────┬───────────┐\n│                     Event │  Thread 1 │  Thread 2 │  Thread 3 │\n├───────────────────────────┼───────────┼───────────┼───────────┤\n│          ACTUAL_CPU_CLOCK │  1.6629e9 │ 4.49548e8 │  4.4838e8 │\n│             MAX_CPU_CLOCK │ 1.15592e9 │ 3.13751e8 │ 3.12692e8 │\n│      RETIRED_INSTRUCTIONS │ 2.18953e9 │ 3.15543e8 │ 3.15447e8 │\n│       CPU_CLOCKS_UNHALTED │ 1.64928e9 │ 4.47128e8 │ 4.47229e8 │\n│ RETIRED_SSE_AVX_FLOPS_ALL │ 1.00035e8 │     1.0e8 │     1.0e8 │\n│                     MERGE │       0.0 │       0.0 │       0.0 │\n└───────────────────────────┴───────────┴───────────┴───────────┘\n┌──────────────────────┬──────────┬──────────┬──────────┐\n│               Metric │ Thread 1 │ Thread 2 │ Thread 3 │\n├──────────────────────┼──────────┼──────────┼──────────┤\n│  Runtime (RDTSC) [s] │  1.03519 │  1.03519 │  1.03519 │\n│ Runtime unhalted [s] │ 0.678736 │  0.18349 │ 0.183013 │\n│          Clock [MHz] │  3524.53 │  3510.38 │  3513.12 │\n│                  CPI │ 0.753256 │  1.41701 │  1.41776 │\n│         DP [MFLOP/s] │   96.634 │  96.6004 │  96.6004 │\n└──────────────────────┴──────────┴──────────┴──────────┘\n","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"","category":"page"},{"location":"howtos/howto_marker_cpu_dynamic/","page":"Marker API (CPU): Dynamic Usage","title":"Marker API (CPU): Dynamic Usage","text":"This page was generated using Literate.jl.","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"EditURL = \"<unknown>/howto_topology.jl\"","category":"page"},{"location":"howtos/howto_topology/#How-CPU-/-NUMA-Topology","page":"System Topology","title":"How  CPU / NUMA Topology","text":"","category":"section"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"The basis functionality of likwid-topology.","category":"page"},{"location":"howtos/howto_topology/#CPU","page":"System Topology","title":"CPU","text":"","category":"section"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"Query CPU topology information:","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"using LIKWID # hide\ntopo = LIKWID.get_cpu_topology()\ntopo.threadPool\ntopo.cacheLevels","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"3-element Vector{LIKWID.CacheLevel}:\n LIKWID.CacheLevel(0, :data, 8, 64, 64, 32768, 1, 0)\n LIKWID.CacheLevel(1, :unified, 8, 1024, 64, 524288, 1, 0)\n LIKWID.CacheLevel(2, :unified, 16, 32768, 64, 33554432, 8, 0)","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"Get detailed CPU information:","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"cpuinfo = LIKWID.get_cpu_info()","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"LIKWID.CpuInfo\n├ family: 25\n├ model: 1\n├ stepping: 1\n├ vendor: 0\n├ part: 0\n├ clock: 2449990641\n├ turbo: false\n├ osname: AMD EPYC 7763 64-Core Processor                \n├ name: AMD K19 (Zen3) architecture\n├ short_name: zen3\n├ features: FP MMX SSE SSE2 HTT MMX RDTSCP MONITOR SSSE FMA SSE4.1 SSE4.2 AES AVX RDRAND AVX2 RDSEED SSE3 \n├ isIntel: false\n├ architecture: \n├ supportUncore: false\n├ supportClientmem: false\n├ featureFlags: 13563999\n├ perf_version: 0\n├ perf_num_ctr: 0\n├ perf_width_ctr: 0\n└ perf_num_fixed_ctr: 0","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"Query information about NUMA nodes:","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"numa = LIKWID.get_numa_topology()\nnuma.nodes\nnuma_node = first(numa.nodes)","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"LIKWID.NumaNode\n├ id: 0\n├ totalMemory: 31.31 GB\n├ freeMemory: 30.31 GB\n├ numberOfProcessors: 16\n├ processors: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n├ numberOfDistances: 8\n└ distances: [10, 12, 12, 12, 32, 32, 32, 32]","category":"page"},{"location":"howtos/howto_topology/#Graphical-output","page":"System Topology","title":"Graphical output","text":"","category":"section"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"Currently, LIKWID.jl doesn't feature a native graphical visualization of the CPU topology. However, it provides a small \"wrapper function\" around likwid-topology -g which should give you an output like this:","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"LIKWID.print_cpu_topology()","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"Graphical Topology\n********************************************************************************\nSocket 0:\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ |\n| |    0   | |    1   | |    2   | |    3   | |    4   | |    5   | |    6   | |    7   | |    8   | |    9   | |   10   | |   11   | |   12   | |   13   | |   14   | |   15   | |   16   | |   17   | |   18   | |   19   | |   20   | |   21   | |   22   | |   23   | |   24   | |   25   | |   26   | |   27   | |   28   | |   29   | |   30   | |   31   | |   32   | |   33   | |   34   | |   35   | |   36   | |   37   | |   38   | |   39   | |   40   | |   41   | |   42   | |   43   | |   44   | |   45   | |   46   | |   47   | |   48   | |   49   | |   50   | |   51   | |   52   | |   53   | |   54   | |   55   | |   56   | |   57   | |   58   | |   59   | |   60   | |   61   | |   62   | |   63   | |\n| +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ |\n| +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ |\n| |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |\n| +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ |\n| +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ |\n| | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | |\n| +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ |\n| +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ |\n| |                                        32 MB                                        | |                                        32 MB                                        | |                                        32 MB                                        | |                                        32 MB                                        | |                                        32 MB                                        | |                                        32 MB                                        | |                                        32 MB                                        | |                                        32 MB                                        | |\n| +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nSocket 1:\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ |\n| |   64   | |   65   | |   66   | |   67   | |   68   | |   69   | |   70   | |   71   | |   72   | |   73   | |   74   | |   75   | |   76   | |   77   | |   78   | |   79   | |   80   | |   81   | |   82   | |   83   | |   84   | |   85   | |   86   | |   87   | |   88   | |   89   | |   90   | |   91   | |   92   | |   93   | |   94   | |   95   | |   96   | |   97   | |   98   | |   99   | |   100  | |   101  | |   102  | |   103  | |   104  | |   105  | |   106  | |   107  | |   108  | |   109  | |   110  | |   111  | |   112  | |   113  | |   114  | |   115  | |   116  | |   117  | |   118  | |   119  | |   120  | |   121  | |   122  | |   123  | |   124  | |   125  | |   126  | |   127  | |\n| +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ |\n| +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ |\n| |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |  32 kB | |\n| +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ |\n| +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ |\n| | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | | 512 kB | |\n| +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ +--------+ |\n| +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ |\n| |                                        32 MB                                        | |                                        32 MB                                        | |                                        32 MB                                        | |                                        32 MB                                        | |                                        32 MB                                        | |                                        32 MB                                        | |                                        32 MB                                        | |                                        32 MB                                        | |\n| +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ +-------------------------------------------------------------------------------------+ |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"","category":"page"},{"location":"howtos/howto_topology/","page":"System Topology","title":"System Topology","text":"This page was generated using Literate.jl.","category":"page"},{"location":"references/marker_gpu/#Marker-API-(GPU)","page":"Marker API (GPU)","title":"Marker API (GPU)","text":"","category":"section"},{"location":"references/marker_gpu/","page":"Marker API (GPU)","title":"Marker API (GPU)","text":"Note: This is a maturing feature. Only NVIDIA GPUs are supported.","category":"page"},{"location":"references/marker_gpu/#Index","page":"Marker API (GPU)","title":"Index","text":"","category":"section"},{"location":"references/marker_gpu/","page":"Marker API (GPU)","title":"Marker API (GPU)","text":"Pages   = [\"marker_gpu.md\"]\nOrder   = [:function, :macro, :type]","category":"page"},{"location":"references/marker_gpu/#API","page":"Marker API (GPU)","title":"API","text":"","category":"section"},{"location":"references/marker_gpu/","page":"Marker API (GPU)","title":"Marker API (GPU)","text":"Modules = [LIKWID.GPUMarker]","category":"page"},{"location":"references/marker_gpu/#LIKWID.GPUMarker.close-Tuple{}","page":"Marker API (GPU)","title":"LIKWID.GPUMarker.close","text":"Close the connection to the LIKWID GPU Marker API and write out measurement data to file. This file will be evaluated by likwid-perfctr.\n\n\n\n\n\n","category":"method"},{"location":"references/marker_gpu/#LIKWID.GPUMarker.getregion-Tuple{AbstractString}","page":"Marker API (GPU)","title":"LIKWID.GPUMarker.getregion","text":"getregion(regiontag::AbstractString) -> nevents, events, time, count\n\nGet the intermediate results of the region identified by regiontag. On success, it returns     * nevents: the number of events in the current group,     * events: a list with all the aggregated event results,     * time: the measurement time for the region and     * count: the number of calls.\n\n\n\n\n\n","category":"method"},{"location":"references/marker_gpu/#LIKWID.GPUMarker.gpuregion-Tuple{Any, AbstractString}","page":"Marker API (GPU)","title":"LIKWID.GPUMarker.gpuregion","text":"gpuregion(f, regiontag::AbstractString)\n\nAdds a LIKWID GPU marker region around the execution of the given function f using GPUMarker.startregion, GPUMarker.stopregion under the hood. Note that LIKWID.GPUMarker.init() and LIKWID.GPUMarker.close() must be called before and after, respectively.\n\nExamples\n\njulia> using LIKWID, CUDA\n\njulia> GPUMarker.init()\n\njulia> gpuregion(\"sleeping...\") do\n           sleep(1)\n       end\ntrue\n\njulia> gpuregion(()->CUDA.rand(100), \"create rand vec\")\ntrue\n\njulia> GPUMarker.close()\n\n\n\n\n\n\n","category":"method"},{"location":"references/marker_gpu/#LIKWID.GPUMarker.init-Tuple{}","page":"Marker API (GPU)","title":"LIKWID.GPUMarker.init","text":"Initialize the NvMon Marker API of the LIKWID library. Must be called previous to all other functions.\n\n\n\n\n\n","category":"method"},{"location":"references/marker_gpu/#LIKWID.GPUMarker.isactive-Tuple{}","page":"Marker API (GPU)","title":"LIKWID.GPUMarker.isactive","text":"Checks whether the NVIDIA GPU Marker API is active, i.e. julia has been started under likwid-perfctr -G ... -W ... -m.\n\n\n\n\n\n","category":"method"},{"location":"references/marker_gpu/#LIKWID.GPUMarker.nextgroup-Tuple{}","page":"Marker API (GPU)","title":"LIKWID.GPUMarker.nextgroup","text":"Switch to the next event set in a round-robin fashion. If you have set only one event set on the command line, this function performs no operation.\n\n\n\n\n\n","category":"method"},{"location":"references/marker_gpu/#LIKWID.GPUMarker.registerregion-Tuple{AbstractString}","page":"Marker API (GPU)","title":"LIKWID.GPUMarker.registerregion","text":"Register a region with name regiontag to the GPU Marker API. On success, true is returned.\n\nThis is an optional function to reduce the overhead of region registration at Marker.startregion. If you don't call registerregion, the registration is done at startregion.\n\n\n\n\n\n","category":"method"},{"location":"references/marker_gpu/#LIKWID.GPUMarker.resetregion-Tuple{AbstractString}","page":"Marker API (GPU)","title":"LIKWID.GPUMarker.resetregion","text":"Reset the values stored using the region name regiontag. On success, true is returned.\n\n\n\n\n\n","category":"method"},{"location":"references/marker_gpu/#LIKWID.GPUMarker.startregion-Tuple{AbstractString}","page":"Marker API (GPU)","title":"LIKWID.GPUMarker.startregion","text":"Start measurements under the name regiontag. On success, true is returned.\n\n\n\n\n\n","category":"method"},{"location":"references/marker_gpu/#LIKWID.GPUMarker.stopregion-Tuple{AbstractString}","page":"Marker API (GPU)","title":"LIKWID.GPUMarker.stopregion","text":"Stop measurements under the name regiontag. On success, true is returned.\n\n\n\n\n\n","category":"method"},{"location":"references/marker_gpu/#LIKWID.GPUMarker.@gpuregion-Tuple{Any, Any}","page":"Marker API (GPU)","title":"LIKWID.GPUMarker.@gpuregion","text":"Convenience macro for flanking code with GPUMarker.startregion and GPUMarker.stopregion.\n\nExamples\n\njulia> using LIKWID, CUDA\n\njulia> GPUMarker.init()\n\njulia> @gpuregion \"sleeping...\" sleep(1)\ntrue\n\njulia> @gpuregion \"create rand vec\" CUDA.rand(100)\ntrue\n\njulia> GPUMarker.close()\n\n\n\n\n\n\n","category":"macro"},{"location":"references/temperature/#CPU-Temperature","page":"CPU temperature","title":"CPU Temperature","text":"","category":"section"},{"location":"references/temperature/#API","page":"CPU temperature","title":"API","text":"","category":"section"},{"location":"references/temperature/","page":"CPU temperature","title":"CPU temperature","text":"Pages   = [\"temperature.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"references/temperature/#Functions","page":"CPU temperature","title":"Functions","text":"","category":"section"},{"location":"references/temperature/","page":"CPU temperature","title":"CPU temperature","text":"LIKWID.init_thermal\nLIKWID.get_temperature","category":"page"},{"location":"references/temperature/#LIKWID.init_thermal","page":"CPU temperature","title":"LIKWID.init_thermal","text":"Initialize thermal measurements on the given CPU.\n\n\n\n\n\n","category":"function"},{"location":"references/temperature/#LIKWID.get_temperature","page":"CPU temperature","title":"LIKWID.get_temperature","text":"Read the current temperature of the given CPU in degrees Celsius.\n\n\n\n\n\n","category":"function"},{"location":"howtos/howto_marker_cpu/","page":"Marker API (CPU)","title":"Marker API (CPU)","text":"EditURL = \"<unknown>/howto_marker_cpu.jl\"","category":"page"},{"location":"howtos/howto_marker_cpu/","page":"Marker API (CPU)","title":"Marker API (CPU)","text":"TODO","category":"page"},{"location":"howtos/howto_marker_cpu/","page":"Marker API (CPU)","title":"Marker API (CPU)","text":"","category":"page"},{"location":"howtos/howto_marker_cpu/","page":"Marker API (CPU)","title":"Marker API (CPU)","text":"This page was generated using Literate.jl.","category":"page"},{"location":"references/misc/#Miscellaneous","page":"Miscellaneous","title":"Miscellaneous","text":"","category":"section"},{"location":"references/misc/#Index","page":"Miscellaneous","title":"Index","text":"","category":"section"},{"location":"references/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Pages   = [\"misc.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"references/misc/#API","page":"Miscellaneous","title":"API","text":"","category":"section"},{"location":"references/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"LIKWID.setverbosity\nLIKWID.gpusupport\nLIKWID.pinmask\nLIKWID.accessmode\nLIKWID.env\nLIKWID.clearenv\nLIKWID.LIKWID_FORCE\nLIKWID.LIKWID_NO_ACCESS\nLIKWID.LIKWID_PIN\nLIKWID.LIKWID_SILENT\nLIKWID.LIKWID_SKIP\nLIKWID.LIKWID_DEBUG\nLIKWID.LIKWID_IGNORE_CPUSET\nLIKWID.LIKWID_FILEPATH\nLIKWID.LIKWID_MODE\nLIKWID.LIKWID_EVENTS\nLIKWID.LIKWID_THREADS\nLIKWID.LIKWID_MPI_CONNECT","category":"page"},{"location":"references/misc/#LIKWID.setverbosity","page":"Miscellaneous","title":"LIKWID.setverbosity","text":"Set the verbosity level of the LIKWID library. Returns true on success.\n\nOptions are:\n\nLIKWID.LibLikwid.DEBUGLEV_ONLY_ERROR or 0\nLIKWID.LibLikwid.DEBUGLEV_INFO or 1\nLIKWID.LibLikwid.DEBUGLEV_DETAIL or 2\nLIKWID.LibLikwid.DEBUGLEV_DEVELOP or 3\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.gpusupport","page":"Miscellaneous","title":"LIKWID.gpusupport","text":"Returns whether LIKWID has been compiled with GPU support (i.e. has been compiled with NVIDIA_INTERFACE=true).\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.pinmask","page":"Miscellaneous","title":"LIKWID.pinmask","text":"pinmask(N::Integer) -> mask\n\nGenerates a mask that can be supplied to likwid pin -s <mask> to pin N Julia threads.\n\nTaken from https://discourse.julialang.org/t/thread-affinitization-pinning-julia-threads-to-cores/58069/8.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.accessmode","page":"Miscellaneous","title":"LIKWID.accessmode","text":"Query the access mode used by LIKWID, i.e. either ACCESSMODE_PERF, ACCESSMODE_DAEMON, or ACCESSMODE_DIRECT.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.env","page":"Miscellaneous","title":"LIKWID.env","text":"List the values of LIKWID_* environment variables.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.clearenv","page":"Miscellaneous","title":"LIKWID.clearenv","text":"Unset all LIKWID_* environment variables (for the current session).\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.LIKWID_FORCE","page":"Miscellaneous","title":"LIKWID.LIKWID_FORCE","text":"Enables the overwriting of counters that are detected to be in-use. The environment variable is similar to the -f/--force command line switch for likwid-perfctr.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.LIKWID_NO_ACCESS","page":"Miscellaneous","title":"LIKWID.LIKWID_NO_ACCESS","text":"The execution does not require the access layer (access to hardware counters). For example, this variable is set by likwid-topology or likwid-pin.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.LIKWID_PIN","page":"Miscellaneous","title":"LIKWID.LIKWID_PIN","text":"The comma-separated list contains the CPUs the application threads should be pinned to. Careful, the first CPU in the cpuset must be the last entry because the application is pinned to this CPU per default.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.LIKWID_SILENT","page":"Miscellaneous","title":"LIKWID.LIKWID_SILENT","text":"Disable stdout output caused by the library and the scripts. Some scripts provide the -q/--quiet command line switch which provides the same functionality.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.LIKWID_SKIP","page":"Miscellaneous","title":"LIKWID.LIKWID_SKIP","text":"Variable content must be a hexmask. This hexmask describes which threads should be skipped while pinning. This function is required to avoid pinning the shepherd threads used by some OpenMP and MPI implementations. The version 4.3.1 introduced an automatic detection of the shepherd threads. In most cases the detection works, but if not, the hexmask overwrites the automatic detection.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.LIKWID_DEBUG","page":"Miscellaneous","title":"LIKWID.LIKWID_DEBUG","text":"Verbosity settings for the LIKWID library.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.LIKWID_IGNORE_CPUSET","page":"Miscellaneous","title":"LIKWID.LIKWID_IGNORE_CPUSET","text":"LIKWID respects the CPUset of the calling process. If you want to measure/run outside of this CPUset, use this environment variable. It will not ignore the CPUset but create a new CPUset internally which contains sysconf(_SC_NPROCESSORS_CONF) hardware threads.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.LIKWID_FILEPATH","page":"Miscellaneous","title":"LIKWID.LIKWID_FILEPATH","text":"Filepath for the result file of the MarkerAPI.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.LIKWID_MODE","page":"Miscellaneous","title":"LIKWID.LIKWID_MODE","text":"Access mode for MarkerAPI. 1 is the code for the access daemon.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.LIKWID_EVENTS","page":"Miscellaneous","title":"LIKWID.LIKWID_EVENTS","text":"Event string or performance group name. Multiple event strings or performance group names can be separated by |.\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.LIKWID_THREADS","page":"Miscellaneous","title":"LIKWID.LIKWID_THREADS","text":"The CPUs LIKWID is configured to run on (comma-separated list).\n\n\n\n\n\n","category":"function"},{"location":"references/misc/#LIKWID.LIKWID_MPI_CONNECT","page":"Miscellaneous","title":"LIKWID.LIKWID_MPI_CONNECT","text":"Connection method for Intel MPI. Default is ssh, see option -r of mpdboot or similar.\n\n\n\n\n\n","category":"function"},{"location":"#LIKWID-Like-I-Knew-What-I'm-Doing","page":"LIKWID","title":"LIKWID - Like I Knew What I'm Doing","text":"","category":"section"},{"location":"","page":"LIKWID","title":"LIKWID","text":"LIKWID.jl is a Julia wrapper for the  performance monitoring and benchmarking suite LIKWID.","category":"page"},{"location":"#Installation","page":"LIKWID","title":"Installation","text":"","category":"section"},{"location":"","page":"LIKWID","title":"LIKWID","text":"Prerequisites:","category":"page"},{"location":"","page":"LIKWID","title":"LIKWID","text":"You must have likwid installed (see the build & install instructions).\nYou must be running Linux. (LIKWID doesn't support macOS or Windows.)","category":"page"},{"location":"","page":"LIKWID","title":"LIKWID","text":"LIKWID.jl is a registered Julia package. Hence, you can simply add it to your Julia environment with the command","category":"page"},{"location":"","page":"LIKWID","title":"LIKWID","text":"] add LIKWID","category":"page"},{"location":"#LIKWID.jl-vs-LinuxPerf.jl","page":"LIKWID","title":"LIKWID.jl vs LinuxPerf.jl","text":"","category":"section"},{"location":"","page":"LIKWID","title":"LIKWID","text":"As per default (and recommendation) LIKWID(.jl) uses a custom access daemon to monitor hardware performance counters. In contrast, LinuxPerf.jl uses Linux's perf_events. However, it is possible to make LIKWID use perf_events as an alternative (inferior) backend. See here for more information.","category":"page"},{"location":"#Supported-CPUs","page":"LIKWID","title":"Supported CPUs","text":"","category":"section"},{"location":"","page":"LIKWID","title":"LIKWID","text":"using LIKWID","category":"page"},{"location":"","page":"LIKWID","title":"LIKWID","text":"LIKWID.print_supported_cpus()\nLibc.flush_cstdio() # hide","category":"page"},{"location":"references/nvmon/#NVIDIA-Monitoring-(NvMon)","page":"NVIDIA monitoring","title":"NVIDIA Monitoring (NvMon)","text":"","category":"section"},{"location":"references/nvmon/","page":"NVIDIA monitoring","title":"NVIDIA monitoring","text":"Note: This is a maturing feature. Only NVIDIA GPUs are supported.","category":"page"},{"location":"references/nvmon/#Index","page":"NVIDIA monitoring","title":"Index","text":"","category":"section"},{"location":"references/nvmon/","page":"NVIDIA monitoring","title":"NVIDIA monitoring","text":"Pages   = [\"nvmon.md\"]\nOrder   = [:function, :macro, :type]","category":"page"},{"location":"references/nvmon/#API","page":"NVIDIA monitoring","title":"API","text":"","category":"section"},{"location":"references/nvmon/","page":"NVIDIA monitoring","title":"NVIDIA monitoring","text":"Modules = [LIKWID.NvMon]","category":"page"},{"location":"references/nvmon/#LIKWID.NvMon.add_event_set-Tuple{AbstractString}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.add_event_set","text":"add_event_set(estr) -> groupid\n\nAdd a performance group or a custom event set to the nvmon module. Returns a groupid (starting at 1) which is required to later specify the event set.\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_event_results-Tuple{Any, Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_event_results","text":"get_event_results([groupid_or_groupname, eventid_or_eventname, gpuid::Integer])\n\nRetrieve the results of monitored events. Same as get_metric_results but for raw events.\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_id_of_active_group-Tuple{}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_id_of_active_group","text":"Return the groupid of the currently activate group.\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_last_metric-Tuple{Integer, Integer, Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_last_metric","text":"Return the derived metric result of the last measurement cycle identified by group groupid and the indices for metric metricidx and gpu gpuid (all starting at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_last_result-Tuple{Integer, Integer, Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_last_result","text":"Return the raw counter register result of the last measurement cycle identified by group groupid and the indices for event eventidx and gpu gpuid (all starting at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_longinfo_of_group-Tuple{Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_longinfo_of_group","text":"Return the (long) description of a performance group with id groupid (starts at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_metric-Tuple{Integer, Integer, Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_metric","text":"Return the derived metric result of all measurements identified by group groupid and the indices for metric metricidx and gpu gpuid (all starting at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_metric_results-Tuple{Any, Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_metric_results","text":"get_metric_results([groupid_or_groupname, metricid_or_metricname, gpuid::Integer])\n\nRetrieve the results of monitored metrics.\n\nOptionally, a group, metric, and gpuid can be provided to select a subset of metrics or a single metric. If given as integers, note that groupid, metricid, and gpuid all start at 1 and the latter enumerates the monitored gpus.\n\nIf no arguments are provided, a nested data structure is returned in which different levels correspond to performance groups, gpus, and metrics (in this order). ```\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_metric_results-Tuple{}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_metric_results","text":"get_metric_results()\n\nGet the metric results for all performance groups and all monitored (NvMon.init) gpus.\n\nReturns a an OrderedDict whose keys correspond to the performance groups and the values hold the results for all monitored gpus. ```\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_name_of_counter-Tuple{Integer, Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_name_of_counter","text":"Return the name of the counter register identified by groupid and eventidx (both start at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_name_of_event-Tuple{Integer, Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_name_of_event","text":"Return the name of the event identified by groupid and eventidx (both start at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_name_of_group-Tuple{Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_name_of_group","text":"Return the name of the group identified by groupid (starts at 1). If it is a custom event set, the name is set to Custom.\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_name_of_metric-Tuple{Integer, Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_name_of_metric","text":"Return the name of a derived metric identified by groupid and metricidx (both start at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_number_of_events-Tuple{Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_number_of_events","text":"Return the number of events in the group with id groupid (starts at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_number_of_gpus-Tuple{}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_number_of_gpus","text":"Return the number of GPUs initialized in the nvmon module.\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_number_of_groups-Tuple{}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_number_of_groups","text":"Return the number of groups currently registered in the nvmon module.\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_number_of_metrics-Tuple{Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_number_of_metrics","text":"Return the number of metrics in the group with id groupid (starts at 1). Always zero for custom event sets.\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_result-Tuple{Integer, Integer, Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_result","text":"Return the raw counter register result of all measurements identified by group groupid and the indices for event eventidx and gpu gpuid (all starting at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_shortinfo_of_group-Tuple{Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_shortinfo_of_group","text":"Return the short information about a performance group with id groupid (starts at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.get_time_of_group-Tuple{Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.get_time_of_group","text":"Return the measurement time for group identified by groupid (starts at 1).\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.isgroupsupported","page":"NVIDIA monitoring","title":"LIKWID.NvMon.isgroupsupported","text":"Checks if the given performance group is available on the given GPU (defaults to the first).\n\n\n\n\n\n","category":"function"},{"location":"references/nvmon/#LIKWID.NvMon.nvmon-Tuple{Any, Any}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.nvmon","text":"nvmon(f, group_or_groups[; gpuids])\n\nMonitor performance groups while executing the given function f on one or multiple GPUs. Note that\n\nNvMon.init and NvMon.finalize are called automatically\nthe measurement of multiple performance groups is sequential and requires multiple executions of f!\n\nKeyword arguments:\n\ngpuids (default: first GPU): specify the GPUs to be monitored\n\nExample\n\njulia> using LIKWID\n\njulia> x = CUDA.rand(1000); y = CUDA.rand(1000);\n\njulia> metrics, events = nvmon(\"FLOPS_DP\") do\n           CUDA.@sync x .+ y;\n       end;\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.read_counters-Tuple{}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.read_counters","text":"Read the counter registers. To be executed after start_counters and before stop_counters. Returns true on success.\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.setup_counters-Tuple{Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.setup_counters","text":"Program the counter registers to measure all events in group groupid (starts at 1). Returns true on success.\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.start_counters-Tuple{}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.start_counters","text":"Start the counter registers. Returns true on success.\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.stop_counters-Tuple{}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.stop_counters","text":"Stop the counter registers. Returns true on success.\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.supported_groups","page":"NVIDIA monitoring","title":"LIKWID.NvMon.supported_groups","text":"Return a dictionary of all available nvmon groups for the GPU identified by gpu (starts at 0).\n\nExamples\n\njulia> NvMon.supported_groups()\nDict{String, LIKWID.GroupInfoCompact} with 4 entries:\n  \"DATA\"     => DATA => Load to store ratio\n  \"FLOPS_SP\" => FLOPS_SP => Single-precision floating point\n  \"FLOPS_HP\" => FLOPS_HP => Half-precision floating point\n  \"FLOPS_DP\" => FLOPS_DP => Double-precision floating point\n\n\n\n\n\n","category":"function"},{"location":"references/nvmon/#LIKWID.NvMon.switch_group-Tuple{Integer}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.switch_group","text":"Switch currently active group to groupid (starts at 1). Returns true on success.\n\n\n\n\n\n","category":"method"},{"location":"references/nvmon/#LIKWID.NvMon.@nvmon-Tuple{Any, Any}","page":"NVIDIA monitoring","title":"LIKWID.NvMon.@nvmon","text":"@nvmon group_or_groups codeblock\n\nSee also: nvmon\n\nExample\n\njulia> using LIKWID\n\njulia> x = CUDA.rand(1000); y = CUDA.rand(1000);\n\njulia> metrics, events = @nvmon \"FLOPS_DP\" x .+ y;\n\n\n\n\n\n","category":"macro"}]
}
